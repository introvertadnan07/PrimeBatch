import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")





import requests
from bs4 import BeautifulSoup
import os

os.makedirs("scraped_data", exist_ok=True)

page_count = 1

while True:
    URL = f"https://quotes.toscrape.com/page/{page_count}/"
    res = requests.get(URL)

    soup = BeautifulSoup(res.text, "lxml")
    quotes = soup.select("div.quote")

    if not quotes:
        print("no valid pages anymore..")
        break

    with open(
        f"scraped_data/quotes{page_count}.html",
        "w",
        encoding="utf-8"
    ) as f:
        f.write(res.text)
        print(f"downloaded data from page {page_count}")

    page_count += 1






with open("scraped_data/quotes1.html", "r", encoding="utf-8") as f:
    html_content = f.read()

soup = BeautifulSoup(html_content, "lxml")


all_quotes = soup.select("div.quote")
life_quotes = []

for q in all_quotes:
    all_tags = []

    for tag in q.select(".tags .tag"):
        all_tags.append(tag.get_text())

    if "life" in all_tags:
        text = q.select_one("span.text").get_text()
        author = q.select_one("small.author").get_text()
        life_quotes.append([text, author])

print(life_quotes)


from bs4 import BeautifulSoup
import os

life_quotes = []

page_count = 1

while True:
    file_path = f"scraped_data/quotes{page_count}.html"

    # stop when file does not exist
    if not os.path.exists(file_path):
        break

    with open(file_path, "r", encoding="utf-8") as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, "lxml")
    all_quotes = soup.select("div.quote")

    for q in all_quotes:
        all_tags = []

        for tag in q.select(".tags .tag"):
            all_tags.append(tag.get_text())

        if "life" in all_tags:
            text = q.select_one("span.text").get_text()
            author = q.select_one("small.author").get_text()
            life_quotes.append([text, author])

    page_count += 1

print(life_quotes)




