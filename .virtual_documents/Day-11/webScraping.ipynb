


import requests


URL = "https://www.scrapethissite.com/pages/simple/"
res = requests.get(URL)


if res.status_code == 200:
    print(res.text)

with open("scraped_data/data1.html", "w", encoding="utf-8") as f:
    f.write(res.text)


if res.status_code == 200:
    print(res.content)


if res.status_code == 200:
    print(res.headers)





from bs4 import BeautifulSoup

with open("scraped_data/data1.html", "r") as f:
    html_content = f.read()

soup = BeautifulSoup(html_content, "lxml")


soup


soup.find("h1")


soup.find("h3")


all_countries = []

all_h3 = soup.find_all("h3")

for h3 in all_h3:
    name = h3.get_text(strip=True)

    country_div = h3.find_parent("div", class_="country")
    population = country_div.select_one("span.country-population").get_text(strip=True)

    all_countries.append({
        "name": name,
        "population": population
    })


print(len(all_countries))
print(all_countries[:3])  # first 3 entries


df = pd.DataFrame(all_countries, columns=["name", "population"])
print(df)


df.to_csv("cleaned_data/data.csv", index=False)



